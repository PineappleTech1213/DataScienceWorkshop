{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32c0b7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhengzeng/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/zhengzeng/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/zhengzeng/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import json, gzip\n",
    "from nltk.corpus import wordnet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "true = True\n",
    "false = False\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "from gensim.test.utils import common_corpus, common_dictionary\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamulticore import LdaMulticor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3a48492a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finish loading review data  2193\n",
      "                                                        0\n",
      "0       I don't use these for their original use, and ...\n",
      "10000   Safety valve leaked water / steam. Returned th...\n",
      "20000   I love this vacuum!  When I first got it, I re...\n",
      "30000   Got these for my husband as a belated Father's...\n",
      "40000   This really has made pouring pancakes much eas...\n",
      "50000   These stainless steel measuring spoons beat pl...\n",
      "60000   This pot is great. My wife was getting on to m...\n",
      "70000   Amazon's product review says these are 7 x 10 ...\n",
      "80000   The gradated shapes are exactly what I needed ...\n",
      "90000   This canner is great!  It works well.  I reall...\n",
      "100000  We juice oranges from 2 trees every year, and ...\n",
      "110000  I bought these basically because I have a smal...\n",
      "120000  I've owned about a dozen pepper mills over the...\n",
      "130000                                         works good\n",
      "140000  I'd prefer to have a newer model microwave, bu...\n",
      "150000  A great thermometer. If only they had stronger...\n",
      "160000  I really was expecting just a plain old  can o...\n",
      "170000  I needed very precise measurements in a pot, a...\n",
      "180000  Use this for my own mix of seasoned salt and i...\n",
      "190000  Love, love, love it. Fries any and everything ...\n"
     ]
    }
   ],
   "source": [
    "#read in data\n",
    "def parse(path):\n",
    "    g = gzip.open(path, 'r')\n",
    "    for l in g:\n",
    "        yield eval(l)\n",
    "\n",
    "data = parse(\"Home_and_Kitchen.json.gz\")\n",
    "\n",
    "        \n",
    "def sub_sample_data(data,data_size):\n",
    "    reviews = {}\n",
    "    index = 0\n",
    "    for d in data:\n",
    "        if index %data_size ==0:\n",
    "            entry = dict(d)\n",
    "            reviews[index] = entry.get('reviewText')\n",
    "        index = index+1\n",
    "    return reviews\n",
    "#     print(dic)\n",
    "reviews = sub_sample_data(data,10000)\n",
    "print(\"finish loading review data \", len(reviews))\n",
    "review_data = pd.DataFrame.from_dict(reviews,orient = \"index\")\n",
    "print(review_data.head(20))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "20c77480",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2193"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.columns = ['reviewText']\n",
    "review_data['text'] = ' '.join(review_data['reviewText'].astype(str))\n",
    "corpus = review_data['text'].values\n",
    "# print(corpus.astype(str))\n",
    "\n",
    "len(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2f09043-0b65-40df-8591-df18b470864b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewText</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Seems a bit expensive for a plastic bottle, bu...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Great product, love it!!</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>This is a sturdy floating corner shelf!  We mo...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I purchased 4 of these shelves. they look grea...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I truly wanted to like this item.  It fell apa...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Pretty flimsy, but does the job. If your corne...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Note, the way this shelf mounts to the wall is...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>It's a shelf.  it was inexpensive and holds a ...</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Instructions are wrong and it is not very sturdy.</td>\n",
       "      <td>I don't use these for their original use, and ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          reviewText  \\\n",
       "0  I don't use these for their original use, and ...   \n",
       "1  Seems a bit expensive for a plastic bottle, bu...   \n",
       "2                           Great product, love it!!   \n",
       "3  This is a sturdy floating corner shelf!  We mo...   \n",
       "4  I purchased 4 of these shelves. they look grea...   \n",
       "5  I truly wanted to like this item.  It fell apa...   \n",
       "6  Pretty flimsy, but does the job. If your corne...   \n",
       "7  Note, the way this shelf mounts to the wall is...   \n",
       "8  It's a shelf.  it was inexpensive and holds a ...   \n",
       "9  Instructions are wrong and it is not very sturdy.   \n",
       "\n",
       "                                                text  \n",
       "0  I don't use these for their original use, and ...  \n",
       "1  I don't use these for their original use, and ...  \n",
       "2  I don't use these for their original use, and ...  \n",
       "3  I don't use these for their original use, and ...  \n",
       "4  I don't use these for their original use, and ...  \n",
       "5  I don't use these for their original use, and ...  \n",
       "6  I don't use these for their original use, and ...  \n",
       "7  I don't use these for their original use, and ...  \n",
       "8  I don't use these for their original use, and ...  \n",
       "9  I don't use these for their original use, and ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "2b54b9ee-cb4c-422c-9975-b96011114322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300/300 [27:30<00:00,  5.50s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The WordNet Lemmatizer\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# Acceptable parts of speech\n",
    "pos_map = {'N': wordnet.NOUN, 'J': wordnet.ADJ, 'V': wordnet.VERB, 'R': wordnet.ADV }\n",
    "\n",
    "# produce a bag of words using lemmatization and tokenization\n",
    "def clean_corpus(list_of_text):\n",
    "    cleaned = []\n",
    "    for i in tqdm(range(300)):\n",
    "        words = []\n",
    "        for sent in nltk.sent_tokenize(list_of_text[i+7]): #sentences in a review \n",
    "            for (word, pos) in nltk.pos_tag(nltk.word_tokenize(sent)):\n",
    "                if not pos[0] in pos_map:\n",
    "                    continue\n",
    "                word = lemmatizer.lemmatize(word.lower(), pos_map[pos[0]])\n",
    "                words.append(word.lower())\n",
    "        cleaned.append(words)\n",
    "    return cleaned\n",
    "\n",
    "# This process will take some time to complete\n",
    "clean_texts = clean_corpus(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "25927738-a71f-4964-951b-18cb1109bead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the first corpus\n",
    "clean_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f1dad6b3-7a64-43fa-9e9f-9d70f9341b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be             948600\n",
      "have           338400\n",
      "do             219600\n",
      "not            186900\n",
      "use            178200\n",
      "very           159900\n",
      "n't            158400\n",
      "great          152100\n",
      "love           134100\n",
      "get            117600\n",
      "so             115500\n",
      "good           111900\n",
      "work           109500\n",
      "make           108600\n",
      "'s             102900\n",
      "just           99900\n",
      "well           93600\n",
      "buy            92700\n",
      "look           91200\n",
      "time           82200\n",
      "up             78000\n",
      "easy           76200\n",
      "nice           74400\n",
      "product        71100\n",
      "more           64800\n",
      "out            57900\n",
      "really         57900\n",
      "quality        57000\n",
      "much           55500\n",
      "need           53100\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Print the 30 most frequent words, and add to stopword list\n",
    "counter = Counter([w for t in clean_texts for w in t])\n",
    "wordcounts = counter.most_common(n=30)\n",
    "stopwords = []\n",
    "for word, count in wordcounts:\n",
    "    stopwords.append(word)\n",
    "    print('%s%i' % (word.ljust(15), count))\n",
    "    \n",
    "# Add some new stopworks to stopword list\n",
    "stopwords.extend(['this', 'it', 'the', 'that','do','n\\'t','item','then','no','not','up','down','on','in','get','even','find','soon','le'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e970edb6-8927-4ccf-8397-cdd377064160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from each lemmatized text block\n",
    "for i in range(len(clean_texts)):\n",
    "    j = 0\n",
    "    while j < len(clean_texts[i]):\n",
    "        if clean_texts[i][j] in stopwords:\n",
    "            del clean_texts[i][j]\n",
    "        else:\n",
    "            j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fabbedc5-4d54-490a-a66d-bf4fc62c04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(clean_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d2cf15b6-3999-4365-bcd2-19cc5e8b1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "dictionary = Dictionary(clean_texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in clean_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2d994d03-253e-47b9-ae10-e662ede010de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "\n",
    "# Pretrain the model using the corpus and number of topics\n",
    "#measure the coherence\n",
    "\n",
    "num_topics = range(10, 30, 2)\n",
    "# Alpha parameter\n",
    "\n",
    "def build_lda_model(corpus, dictionary,text, num_topic):\n",
    "    lda = LdaMulticore(corpus, id2word=dictionary, num_topics=num_topic, passes=2)\n",
    "    cm = CoherenceModel(lda, corpus=corpus, dictionary=dictionary, texts =text,coherence='c_v')\n",
    "    coherence = cm.get_coherence()\n",
    "    print(\"for model with %d topics: coherence is %f\" %(num_topic,coherence))\n",
    "    return lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca7311-c796-42a3-9119-5725306b47f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for model with 10 topics: coherence is 0.238590\n",
      "for model with 12 topics: coherence is 0.238076\n",
      "for model with 14 topics: coherence is 0.233729\n",
      "for model with 16 topics: coherence is 0.240597\n"
     ]
    }
   ],
   "source": [
    "#re run the new data into the model, so the coherence result may not the same as in the txt file. But they show the same trend.\n",
    "models = []\n",
    "for num in num_topics:\n",
    "    model = build_lda_model(corpus,dictionary,clean_texts,num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "90001add-0e66-477c-9fe0-d5ccbe865fc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for model with 28 topics: coherence is 0.372404\n"
     ]
    }
   ],
   "source": [
    "ldamodel = build_lda_model(corpus,dictionary,clean_texts,28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "289bc575-4e3a-4b1e-a667-22061773c8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: 0.016*\"find\" + 0.014*\"issue\" + 0.014*\"hard\" + 0.012*\"place\" + 0.011*\"nice\" + 0.011*\"really\" + 0.010*\"long\" + 0.010*\"anything\" + 0.010*\"take\" + 0.009*\"size\"\n",
      "\n",
      "Topic 1: 0.019*\"make\" + 0.017*\"cover\" + 0.013*\"easy\" + 0.013*\"tea\" + 0.013*\"lid\" + 0.012*\"best\" + 0.011*\"take\" + 0.010*\"hard\" + 0.010*\"page\" + 0.009*\"new\"\n",
      "\n",
      "Topic 2: 0.017*\"coffee\" + 0.016*\"nice\" + 0.009*\"keep\" + 0.009*\"hot\" + 0.008*\"new\" + 0.008*\"size\" + 0.008*\"reminder\" + 0.007*\"much\" + 0.007*\"too\" + 0.007*\"price\"\n",
      "\n",
      "Topic 3: 0.014*\"'m\" + 0.014*\"daughter\" + 0.012*\"little\" + 0.012*\"page\" + 0.011*\"quality\" + 0.011*\"come\" + 0.011*\"expect\" + 0.009*\"make\" + 0.009*\"uderzo\" + 0.009*\"get\"\n",
      "\n",
      "Topic 4: 0.016*\"make\" + 0.015*\"more\" + 0.013*\"keep\" + 0.012*\"nativity\" + 0.012*\"order\" + 0.011*\"http\" + 0.011*\"poster\" + 0.011*\"small\" + 0.011*\"go\" + 0.008*\"friend\"\n",
      "\n",
      "Topic 5: 0.017*\"cup\" + 0.014*\"star\" + 0.013*\"out\" + 0.010*\"family\" + 0.010*\"want\" + 0.010*\"come\" + 0.010*\"one\" + 0.010*\"more\" + 0.009*\"next\" + 0.009*\"get\"\n",
      "\n",
      "Topic 6: 0.013*\"series\" + 0.012*\"much\" + 0.010*\"easy\" + 0.010*\"like\" + 0.009*\"alien\" + 0.008*\"space\" + 0.008*\"one\" + 0.008*\"goscinny\" + 0.007*\"child\" + 0.007*\"here\"\n",
      "\n",
      "Topic 7: 0.019*\"....\" + 0.017*\"order\" + 0.017*\"work\" + 0.014*\"gift\" + 0.012*\"way\" + 0.012*\"akiane\" + 0.012*\"god\" + 0.012*\"jesus\" + 0.012*\"someone\" + 0.011*\"know\"\n",
      "\n",
      "Topic 8: 0.012*\"move\" + 0.011*\"see\" + 0.011*\"fun\" + 0.011*\"come\" + 0.011*\"seem\" + 0.011*\"go\" + 0.010*\"something\" + 0.010*\"alien\" + 0.010*\"fall\" + 0.007*\"expect\"\n",
      "\n",
      "Topic 9: 0.017*\"family\" + 0.013*\"judy\" + 0.013*\"jesus\" + 0.013*\"tell\" + 0.012*\"size\" + 0.011*\"wallet\" + 0.009*\"learn\" + 0.009*\"figure\" + 0.009*\"piece\" + 0.009*\"christmas\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('Topic %i: %s\\n' % (i, ldamodel.print_topic(i, topn=10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a78cff81-8c4f-42a9-b380-d00db8daad2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./topic.model.pkl']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#saving the model\n",
    "import joblib\n",
    "joblib.dump(ldamodel,'./topic.model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
