{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><h2>Machine Learning Model Lab and Excercise</h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Overview</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>\n",
    "    <li>Lab</li>\n",
    "    <li>ML Excercise</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Reminder: Process Flow for Big-Data Machine Learning Modeling</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><figure><img src=\"http://stat.cmu.edu/~mfarag/14810/l15/machine_learning_modeling_lifecycle_in_big_data.png\"/><figcaption>Process Flow for Big-Data Machine Learning Modeling</figcaption></figure></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Deploy Your Tuned ML Model to the Cloud</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<ul>In order to run your ML model to the cloud, you need to conduct the following steps:\n",
    "    <li>Enable the Billing</li>\n",
    "    <li>Start the Cluster</li>\n",
    "    <li>Upload your data files in one folder to the GCP bucket</li>\n",
    "    <li>Move the folder containing the data from the GCP bucket to your local cluster</li>\n",
    "    <li>Move the folder containing the data from your local cluster to HDFS</li>\n",
    "    <li>Upload your Jupyter notebook(s) to the cluster</li>\n",
    "    <li>Update the Path for your data files in the Jupyter Notebook</li>\n",
    "    <li>Run The Model in the Notebook</li>\n",
    "    <li>Stop the Cluster after you finish</li>\n",
    "    <li>When the Cluster is stopped, disable your billing</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Today's Problem</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<b>In this lecture, work in groups of 2 to develop ML model to predict the result of a given play (i.e. PlayResult column).</b>\n",
    "<ul>\n",
    "    <li>All the steps are implemented for you up-to the Data Scaling Phase.</li>\n",
    "    <li>Start by deciding if this is regression or classification problem</li>\n",
    "    <li>Depending on the type of the problem, use two ML models and try to identify which one of them is better (e.g. if it's regression, use two ML models. If it's classification, use two ML models. Check which one has better accuracy than the other</li>\n",
    "    <li>When comparing the two Models, you may use the following approach:<ul>\n",
    "        <li>Use the one with the highest <b>R-squared</b> if they are regression models</li>\n",
    "        <li>Use the one with the highest AUC if they are classification models</li></ul></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of having our regular quiz at the end of this week, you will work on this problem in a group of 2 and inform me about your progress by the end of the lecture. Today's quiz has total of 2 points.\n",
    "<ul>\n",
    "<li>You will get 1 point for today's quiz by trying to solve the problem</li>\n",
    "<li>You will get 2 points if you developed 2 ML models and told me which ones are better to correctly solve the problem and ran it on the cloud</li>\n",
    "</ul>\n",
    "I will release working solution at the end of the lecture for your reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Feature Engineering is Done for you!</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Before we start, let's go ahead and conduct our basic data loading and cleaning\n",
    "<ul>\n",
    "    <li>Start Your Spark Session</li>\n",
    "    <li>Ingest your data into the application</li>\n",
    "    <li>Perform required data cleaning</li>\n",
    "</ul>\n",
    "<br/>\n",
    "For simplicity, I'll ingest the data right from the CSVs. Ideally, you would ingest your data from PostgreSQL and join the dataframes back into one big dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- GameClock: string (nullable = true)\n",
      " |-- yardsToGo: integer (nullable = true)\n",
      " |-- possessionTeam: string (nullable = true)\n",
      " |-- yardlineSide: string (nullable = true)\n",
      " |-- offenseFormation: string (nullable = true)\n",
      " |-- personnel_offense: string (nullable = true)\n",
      " |-- personnel_defense: string (nullable = true)\n",
      " |-- HomeScoreBeforePlay: integer (nullable = true)\n",
      " |-- VisitorScoreBeforePlay: integer (nullable = true)\n",
      " |-- HomeScoreAfterPlay: integer (nullable = true)\n",
      " |-- VisitorScoreAfterPlay: integer (nullable = true)\n",
      " |-- isPenalty: boolean (nullable = true)\n",
      " |-- isSTPlay: boolean (nullable = true)\n",
      " |-- SpecialTeamsPlayType: string (nullable = true)\n",
      " |-- PassResult: string (nullable = true)\n",
      " |-- PlayResult: integer (nullable = true)\n",
      " |-- playDescription: string (nullable = true)\n",
      " |-- yardline_number: integer (nullable = true)\n",
      " |-- defenders_in_the_box: integer (nullable = true)\n",
      " |-- number_of_pass_rushers: integer (nullable = true)\n",
      " |-- kick_return_yardage: integer (nullable = true)\n",
      " |-- pass_length: integer (nullable = true)\n",
      " |-- yards_after_catch: integer (nullable = true)\n",
      " |-- play_id: string (nullable = true)\n",
      " |-- game_id: string (nullable = true)\n",
      " |-- quarter_var: string (nullable = true)\n",
      " |-- down_var: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# if you installed Spark on windows, \n",
    "# you may need findspark and need to initialize it prior to being able to use pyspark\n",
    "# Also, you may need to initialize SparkContext yourself.\n",
    "#import findspark\n",
    "#findspark.find()\n",
    "#findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "\n",
    "appName = \"Machine Learning via SparkML\"\n",
    "master = \"local\"\n",
    "\n",
    "# Create Configuration object for Spark.\n",
    "conf = pyspark.SparkConf()\\\n",
    "    .set('spark.driver.host','127.0.0.1')\\\n",
    "    .setAppName(appName)\\\n",
    "    .setMaster(master)\n",
    "\n",
    "# Create Spark Context with the new configurations rather than rely on the default one\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "\n",
    "# You need to create SQL Context to conduct some database operations like what we will see later.\n",
    "#sqlContext = SQLContext(sc)\n",
    "\n",
    "# If you have SQL context, you create the session from the Spark Context\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "#Ingest data from the players CSV into Spark Dataframe. What is dataframe?\n",
    "plays_df = (spark.read\n",
    "         .format(\"csv\")\n",
    "         .option(\"inferSchema\", \"true\")\n",
    "         .option(\"header\",\"true\")\n",
    "         .load(\"/Data/plays.csv\")\n",
    "      )\n",
    "\n",
    "renamed_columns_plays_df = plays_df.withColumnRenamed(\"personnelOffense\",\"personnel_offense\").withColumnRenamed(\"personnelDefense\",\"personnel_defense\")\n",
    "casted_types_df = (renamed_columns_plays_df\n",
    "              .withColumn(\"yardline_number\", renamed_columns_plays_df[\"yardlineNumber\"].cast(\"integer\")).drop(\"yardlineNumber\")\n",
    "              .withColumn(\"defenders_in_the_box\", renamed_columns_plays_df[\"defendersInTheBox\"].cast(\"integer\")).drop(\"defendersInTheBox\")\n",
    "              .withColumn(\"number_of_pass_rushers\", renamed_columns_plays_df[\"numberOfPassRushers\"].cast(\"integer\")).drop(\"numberOfPassRushers\")\n",
    "              .withColumn(\"kick_return_yardage\", renamed_columns_plays_df[\"KickReturnYardage\"].cast(\"integer\")).drop(\"KickReturnYardage\")\n",
    "              .withColumn(\"pass_length\", renamed_columns_plays_df[\"PassLength\"].cast(\"integer\")).drop(\"PassLength\")     \n",
    "              .withColumn(\"yards_after_catch\", renamed_columns_plays_df[\"YardsAfterCatch\"].cast(\"integer\")).drop(\"YardsAfterCatch\")     \n",
    "              .withColumn(\"play_id\", renamed_columns_plays_df[\"playId\"].cast(\"string\")).drop(\"playId\")     \n",
    "              .withColumn(\"game_id\", renamed_columns_plays_df[\"gameId\"].cast(\"string\")).drop(\"gameId\")     \n",
    "              .withColumn(\"quarter_var\", renamed_columns_plays_df[\"quarter\"].cast(\"string\")).drop(\"quarter\")     \n",
    "              .withColumn(\"down_var\", renamed_columns_plays_df[\"down\"].cast(\"string\")).drop(\"down\")     \n",
    "            .distinct()\n",
    "           )\n",
    "plays_df_with_substituted_na = (casted_types_df\\\n",
    "    .withColumn('offenseFormation', \\\n",
    "                when(casted_types_df.offenseFormation=='NA',regexp_replace(casted_types_df.offenseFormation,'NA',None)) \\\n",
    "                .otherwise(casted_types_df.offenseFormation))\\\n",
    "    .withColumn('personnel_offense', \\\n",
    "                when(casted_types_df.personnel_offense=='NA',regexp_replace(casted_types_df.personnel_offense,'NA',None)) \\\n",
    "                .otherwise(casted_types_df.personnel_offense))\\\n",
    "    .withColumn('personnel_defense', \\\n",
    "                when(casted_types_df.personnel_defense=='NA',regexp_replace(casted_types_df.personnel_defense,'NA',None)) \\\n",
    "                .otherwise(casted_types_df.personnel_defense))\n",
    "    .withColumn('SpecialTeamsPlayType', \\\n",
    "                when(casted_types_df.SpecialTeamsPlayType=='NA',regexp_replace(casted_types_df.SpecialTeamsPlayType,'NA',None)) \\\n",
    "                .otherwise(casted_types_df.SpecialTeamsPlayType))\n",
    "    .withColumn('PassResult', \\\n",
    "                when(casted_types_df.PassResult=='NA',regexp_replace(casted_types_df.PassResult,'NA',None)) \\\n",
    "                .otherwise(casted_types_df.PassResult)))\n",
    "\n",
    "numeric_features = [feature[0] for feature in plays_df_with_substituted_na.dtypes if feature[1] == 'int']\n",
    "\n",
    "plays_df_with_substituted_na.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/04/12 15:48:16 WARN org.apache.spark.sql.catalyst.util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|PlayResult|\n",
      "+----------+\n",
      "|       -35|\n",
      "|        31|\n",
      "|        65|\n",
      "|        53|\n",
      "|        78|\n",
      "|       -13|\n",
      "|       -20|\n",
      "|        34|\n",
      "|        -1|\n",
      "|       -17|\n",
      "|        28|\n",
      "|        26|\n",
      "|        27|\n",
      "|       -10|\n",
      "|        44|\n",
      "|       -11|\n",
      "|        12|\n",
      "|        22|\n",
      "|       -15|\n",
      "|        47|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "plays_df_with_substituted_na.select(\"PlayResult\").distinct().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>How would you create the outcome variable?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plays_df_with_outcome_var = plays_df_with_substituted_na\\\n",
    "                        .withColumn('target', renamed_columns_plays_df[\"PlayResult\"].cast(\"string\")).drop(\"PlayResult\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "filtered_plays_df = plays_df_with_outcome_var.select(\n",
    "\"yardsToGo\",\n",
    "\"possessionTeam\",\n",
    "\"yardlineSide\",\n",
    "\"offenseFormation\",\n",
    "\"personnel_offense\",\n",
    "\"personnel_defense\",\n",
    "\"isSTPlay\",\n",
    "\"yardline_number\",\n",
    "\"defenders_in_the_box\",\n",
    "\"number_of_pass_rushers\",\n",
    "\"down_var\",\n",
    "\"play_id\",\n",
    "\"game_id\",\n",
    "\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's continue our feature engineering from last class (without changes).\n",
    "<h2>Process Continuous Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0----------------------\n",
      " yardsToGo              | 0    \n",
      " yardline_number        | 180  \n",
      " defenders_in_the_box   | 2637 \n",
      " number_of_pass_rushers | 7483 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handle Null Values\n",
    "numeric_columns = [column[0] for column in filtered_plays_df.dtypes if column[1]=='int']\n",
    "filtered_plays_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in numeric_columns]).show(vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Impute numerical fields with median\n",
    "list_to_be_imputed = ['yardline_number','defenders_in_the_box','number_of_pass_rushers']\n",
    "plays_df_filled_na = filtered_plays_df.fillna(-200, list_to_be_imputed)\n",
    "\n",
    "imputer = Imputer (\n",
    "            inputCols=list_to_be_imputed,\n",
    "            outputCols=[\"{}_imputed\".format(c) for c in list_to_be_imputed])\\\n",
    "                .setStrategy(\"median\").setMissingValue(-200)\n",
    "\n",
    "plays_df_imputed = imputer.fit(plays_df_filled_na).transform(plays_df_filled_na)\n",
    "plays_df_imputed_enhanced = plays_df_imputed.drop('yardline_number','defenders_in_the_box','number_of_pass_rushers')\n",
    "\n",
    "renamed_plays_df_imputed = plays_df_imputed_enhanced.withColumnRenamed(\"yardline_number_imputed\",\"yardline_number\")\\\n",
    "                            .withColumnRenamed(\"defenders_in_the_box_imputed\",\"defenders_in_the_box\")\\\n",
    "                            .withColumnRenamed(\"number_of_pass_rushers_imputed\",\"number_of_pass_rushers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "non_correlated_plays_df = renamed_plays_df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Handling Outliers</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "def column_add(a,b):\n",
    "     return  a.__add__(b)\n",
    "    \n",
    "def find_outliers(df):\n",
    "    # Identifying the numerical columns in a spark dataframe\n",
    "    numeric_columns = [column[0] for column in df.dtypes if column[1]=='int']\n",
    "\n",
    "    # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "    for column in numeric_columns:\n",
    "\n",
    "        less_Q1 = 'less_Q1_{}'.format(column)\n",
    "        more_Q3 = 'more_Q3_{}'.format(column)\n",
    "        Q1 = 'Q1_{}'.format(column)\n",
    "        Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "        # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "        Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "        Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "        \n",
    "        # IQR : Inter Quantile Range\n",
    "        # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "        # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "        IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "        #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "        less_Q1 =  Q1[0] - 1.5*IQR\n",
    "        more_Q3 =  Q3[0] + 1.5*IQR\n",
    "        \n",
    "        isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "        \n",
    "        df = df.withColumn(isOutlierCol,when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "    \n",
    "\n",
    "    # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "    selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "    # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "    df = df.withColumn('total_outliers',reduce(column_add, ( df[col] for col in  selected_columns)))\n",
    "\n",
    "    # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "    df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0---------------------\n",
      " yardsToGo              | 0   \n",
      " yardline_number        | 0   \n",
      " defenders_in_the_box   | 0   \n",
      " number_of_pass_rushers | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a reminder, we don't have any null values for the outliers to be handled\n",
    "numeric_columns = [column[0] for column in non_correlated_plays_df.dtypes if column[1]=='int']\n",
    "non_correlated_plays_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in numeric_columns]).show(vertical=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plays_df_with_substituted_na_and_outliers = find_outliers(non_correlated_plays_df)\n",
    "plays_df_with_handled_outliers = plays_df_with_substituted_na_and_outliers.filter(plays_df_with_substituted_na_and_outliers['total_Outliers']<1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Handle Binary Variables (by casting them)</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------------+\n",
      "|isSTPlay|isSTPlay_encoded|\n",
      "+--------+----------------+\n",
      "|    true|               1|\n",
      "|   false|               0|\n",
      "+--------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "plays_df_with_handled_binary = (plays_df_with_handled_outliers\n",
    "              .withColumn(\"isSTPlay_encoded\", plays_df_with_handled_outliers[\"isSTPlay\"].cast(\"integer\")))\n",
    "plays_df_with_handled_binary.select(\"isSTPlay\",\"isSTPlay_encoded\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Handle Nominal Variables</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# define stage 1 : transform the columns to numeric\n",
    "stage_1 = StringIndexer(inputCol= 'possessionTeam', outputCol= 'possessionTeam_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_2 = StringIndexer(inputCol= 'personnel_offense', outputCol= 'personnel_offense_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_3 = StringIndexer(inputCol= 'personnel_defense', outputCol= 'personnel_defense_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_4 = StringIndexer(inputCol= 'offenseFormation', outputCol= 'offenseFormation_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_5 = StringIndexer(inputCol= 'yardlineSide', outputCol= 'yardlineSide_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_6 = StringIndexer(inputCol= 'down_var', outputCol= 'down_index', handleInvalid=\"keep\")\n",
    "\n",
    "\n",
    "# define stage 4 : one hot encode the numeric columns\n",
    "stage_7 = OneHotEncoder(inputCols=['possessionTeam_index','personnel_offense_index','personnel_defense_index',\n",
    "                                  'offenseFormation_index','yardlineSide_index', 'down_index'], \n",
    "                        outputCols=['possessionTeam_encoded','personnel_offense_encoded','personnel_defense_encoded',\n",
    "                                   'offenseFormation_encoded','yardlineSide_encoded', 'down_encoded'])\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5, stage_6, stage_7])\n",
    "\n",
    "# fit the pipeline model and transform the data as defined\n",
    "pipeline_model = pipeline.fit(plays_df_with_handled_binary)\n",
    "encoded_plays_df = pipeline_model.transform(plays_df_with_handled_binary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Combining Features into Single Vector</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=['possessionTeam_encoded','personnel_offense_encoded','personnel_defense_encoded',\n",
    "               'offenseFormation_encoded','yardlineSide_encoded','isSTPlay_encoded',\n",
    "               'yardsToGo', \"yardline_number\", \"defenders_in_the_box\",\"number_of_pass_rushers\"], \n",
    "    outputCol=\"vectorized_features\")\n",
    "\n",
    "assembled_plays_df = vector_assembler.transform(encoded_plays_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Data Scaling</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|target|\n",
      "+------+\n",
      "|     0|\n",
      "|     8|\n",
      "|    40|\n",
      "|    -6|\n",
      "|    40|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "standard_scaler = StandardScaler(inputCol= 'vectorized_features', outputCol= 'features')\n",
    "scaled_model = standard_scaler.fit(assembled_plays_df)\n",
    "scaled_plays_df = scaled_model.transform(assembled_plays_df)\n",
    "\n",
    "scaled_plays_df.select(\"target\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2>Now, let's Create the ML Model</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "### Your work Start Here!!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
