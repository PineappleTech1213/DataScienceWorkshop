{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "207de267",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Spark/spark-3.2.1-bin-hadoop3.2-scala2.13/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#import \n",
    "import pyspark\n",
    "from pyspark import SparkContext #for unabling to set up sc by yourself\n",
    "from pyspark.sql import SparkSession, SQLContext #spark dataframe = spark sql\n",
    "from pyspark.sql.functions import to_date, col, count,when,lit,rand,to_timestamp,udf\n",
    "from pyspark.sql.functions import concat_ws, substring,isnan,regexp_replace,concat\n",
    "from pyspark.sql.functions import udf,regexp_extract, monotonically_increasing_id\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "#building spark session\n",
    "appname = \"homework4\" #define app name\n",
    "master = \"local\"\n",
    "\n",
    "config = pyspark.SparkConf().setAppName(appname)\\\n",
    ".setMaster(master) #we do not have any workers.\n",
    "\n",
    "\n",
    "#session\n",
    "#with sql context, create session from it\n",
    "sc  = SparkContext.getOrCreate(conf=config)\n",
    "sqlContext = SQLContext(sc)\n",
    "#don't create many sessions, take up too much room!\n",
    "sp_session = sqlContext.sparkSession.builder.getOrCreate();\n",
    "#export data into postgresql\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "\n",
    "def read_data(tablename):\n",
    "    play_df = sqlContext.read.format(\"jdbc\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\\\n",
    "    .option(\"dbtable\", tablename)\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"postgres\")\\\n",
    "    .option(\"Driver\", \"org.postgresql.Driver\")\\\n",
    "    .load()\n",
    "    play_df.printSchema()\n",
    "    return play_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1870759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- gameid: string (nullable = true)\n",
      " |-- playid: string (nullable = true)\n",
      " |-- quarter: integer (nullable = true)\n",
      " |-- gameclock: timestamp (nullable = true)\n",
      " |-- down: integer (nullable = true)\n",
      " |-- yardsToGo: integer (nullable = true)\n",
      " |-- possessionTeam: string (nullable = true)\n",
      " |-- yardlineSide: string (nullable = true)\n",
      " |-- yardlineNumber: string (nullable = true)\n",
      " |-- offenseFormation: string (nullable = true)\n",
      " |-- personnel_off: string (nullable = true)\n",
      " |-- personnel_def: string (nullable = true)\n",
      " |-- isPenalty: boolean (nullable = true)\n",
      " |-- isSTPlay: boolean (nullable = true)\n",
      " |-- PassResult: string (nullable = true)\n",
      " |-- PlayResult: integer (nullable = true)\n",
      " |-- Pass_Length: integer (nullable = true)\n",
      " |-- YardAfterCatch: integer (nullable = true)\n",
      " |-- defenders_InTheBox: integer (nullable = true)\n",
      " |-- number_PassRushers: integer (nullable = true)\n",
      " |-- Play_Description: string (nullable = true)\n",
      " |-- Player_1: string (nullable = true)\n",
      " |-- Player_2: string (nullable = true)\n",
      " |-- Player_3: string (nullable = true)\n",
      " |-- SpTeamPlayId: string (nullable = true)\n",
      " |-- Home_Score_ID: string (nullable = true)\n",
      " |-- Visitor_Score_ID: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- gameid: string (nullable = true)\n",
      " |-- season: integer (nullable = true)\n",
      " |-- week: integer (nullable = true)\n",
      " |-- game_date: date (nullable = true)\n",
      " |-- game_time_est: timestamp (nullable = true)\n",
      " |-- HomeScore: integer (nullable = true)\n",
      " |-- VisitorScore: integer (nullable = true)\n",
      " |-- homeTeamAbbr: string (nullable = true)\n",
      " |-- visitorTeamAbbr: string (nullable = true)\n",
      " |-- Location_ID: string (nullable = true)\n",
      " |-- GameWeather_ID: string (nullable = true)\n",
      " |-- GameLength: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- SpecialTeamsPlayType: string (nullable = true)\n",
      " |-- KickReturn: integer (nullable = true)\n",
      " |-- SpTeamPlayId: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#ingest data from previous play data in postgres\n",
    "\n",
    "plays_df =read_data(\"NFL.PLAYS\")\n",
    "games= read_data(\"NFL.GAMES\")\n",
    "team_plays = read_data(\"NFL.SPECIAL_TM_PLAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95cc8238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|PassResult|\n",
      "+----------+\n",
      "|        NA|\n",
      "|         C|\n",
      "|        IN|\n",
      "|         S|\n",
      "|         R|\n",
      "|         I|\n",
      "+----------+\n",
      "\n",
      "root\n",
      " |-- playid: string (nullable = true)\n",
      " |-- gameid2: string (nullable = true)\n",
      " |-- IPcount: long (nullable = false)\n",
      " |-- SpecialTeamsPlayType: string (nullable = true)\n",
      " |-- possessionTeam: string (nullable = true)\n",
      " |-- isSTPlay: boolean (nullable = true)\n",
      " |-- offenseFormation: string (nullable = true)\n",
      " |-- personnel_off: string (nullable = true)\n",
      " |-- personnel_def: string (nullable = true)\n",
      " |-- yardlineSide: string (nullable = true)\n",
      " |-- KickReturn: integer (nullable = true)\n",
      " |-- number_PassRushers: integer (nullable = true)\n",
      " |-- defenders_InTheBox: integer (nullable = true)\n",
      " |-- YardAfterCatch: integer (nullable = true)\n",
      " |-- Pass_Length: integer (nullable = true)\n",
      " |-- yardsToGo: integer (nullable = true)\n",
      " |-- HomeScore: integer (nullable = true)\n",
      " |-- VisitorScore: integer (nullable = true)\n",
      " |-- yardline_number: integer (nullable = true)\n",
      "\n",
      "+-------+-----+\n",
      "|IPcount|count|\n",
      "+-------+-----+\n",
      "|      7|  868|\n",
      "|      6|  567|\n",
      "|      9|  998|\n",
      "|      5|  533|\n",
      "|     10|  824|\n",
      "|     12| 1069|\n",
      "|      8|  862|\n",
      "|     11| 1202|\n",
      "|      4|  355|\n",
      "|     13|  414|\n",
      "|     14| 1033|\n",
      "|     15|  870|\n",
      "|     16|  427|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#join plays with team play type data\n",
    "plays_df.select(\"PassResult\").distinct().show()\n",
    "plays_with_teamplay = plays_df.join(team_plays,plays_df.SpTeamPlayId  == team_plays.SpTeamPlayId).drop(\"SpTeamPlayId\")\n",
    "#rename the id in games data\n",
    "renamed_games = games.withColumnRenamed('gameid','gameid2')\\\n",
    ".withColumn(\"gamelength\", col(\"GameLength\").cast(\"int\")).drop(\"GameLength\")\n",
    "\n",
    "# renamed_games.printSchema()\n",
    "#join plays with games \n",
    "plays_with_games = plays_with_teamplay.join(renamed_games, renamed_games.gameid2 == plays_with_teamplay.gameid)\n",
    "# plays_with_games.show(1,vertical=True)\n",
    "#calculate games' incomplete pass\n",
    "games_pass = plays_with_games.select(\"gameid\",\"PassResult\")\\\n",
    ".groupBy(col(\"gameid\")).agg(count(when(col(\"PassResult\") == 'I',1)).alias(\"IPcount\"))\n",
    "# games_pass.show(1,vertical=True)\n",
    "\n",
    "#join the pass result count with plays_with_games\n",
    "games_df = plays_with_games.join(games_pass,plays_with_games.gameid == games_pass.gameid).drop('gameid')\\\n",
    ".select(\"playid\",\"gameid2\",\"IPcount\",\"SpecialTeamsPlayType\",\"possessionTeam\",\"isSTPlay\",\"offenseFormation\",\n",
    "        \"personnel_off\",\"personnel_def\",\"yardlineSide\",\n",
    "        \"yardlineNumber\",\"KickReturn\",\"number_PassRushers\",\"defenders_InTheBox\",\"YardAfterCatch\",\n",
    "       \"Pass_Length\",\"yardsToGo\",\"HomeScore\",\"VisitorScore\").withColumn(\"yardline_number\", col(\"yardLineNumber\").cast(\"int\")).drop(\"yardLineNumber\")\n",
    "games_df.printSchema()\n",
    "#see the distribution of the incomplete pass count\n",
    "games_df.groupBy(\"IPcount\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3b5e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0-----------------\n",
      " KickReturn         | 0   \n",
      " number_PassRushers | 0   \n",
      " defenders_InTheBox | 0   \n",
      " YardAfterCatch     | 0   \n",
      " Pass_Length        | 0   \n",
      " yardsToGo          | 0   \n",
      " HomeScore          | 0   \n",
      " VisitorScore       | 0   \n",
      " yardline_number    | 83  \n",
      "\n",
      "-RECORD 0---------------------\n",
      " KickReturn             | 0   \n",
      " number_PassRushers     | 0   \n",
      " defenders_InTheBox     | 0   \n",
      " YardAfterCatch         | 0   \n",
      " Pass_Length            | 0   \n",
      " yardsToGo              | 0   \n",
      " HomeScore              | 0   \n",
      " VisitorScore           | 0   \n",
      " yardlin_number_imputed | 0   \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#start data cleaning process \n",
    "#make sure there is no null value\n",
    "\n",
    "numeric_cols = [column[0] for column in games_df.dtypes if column[1]==\"int\"]\n",
    "games_df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in numeric_cols]).show(vertical=True)\n",
    "#yardline_number is not imputed\n",
    "games_df_filled_na = games_df.fillna(-200, 'yardline_number')\n",
    "\n",
    "imputer = Imputer (\n",
    "            inputCol='yardline_number',\n",
    "            outputCol='yardlin_number_imputed')\\\n",
    "                .setStrategy(\"median\").setMissingValue(-200)\n",
    "\n",
    "games_df_imputed = imputer.fit(games_df_filled_na).transform(games_df_filled_na).drop('yardline_number')\n",
    "\n",
    "\n",
    "numeric_cols = [column[0] for column in games_df_imputed.dtypes if column[1]==\"int\"]\n",
    "games_df_imputed.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in numeric_cols]).show(vertical=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dfb47c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#deal with outliers\n",
    "#here is to deal with outliers with codes from lectures\n",
    "from functools import reduce\n",
    "\n",
    "def column_add(a,b):\n",
    "     return  a.__add__(b)\n",
    "    \n",
    "def find_outliers(df):\n",
    "    # Identifying the numerical columns in a spark dataframe\n",
    "    numeric_columns = [column[0] for column in df.dtypes if column[1]=='int']\n",
    "\n",
    "    # Using the `for` loop to create new columns by identifying the outliers for each feature\n",
    "    for column in numeric_columns:\n",
    "\n",
    "        less_Q1 = 'less_Q1_{}'.format(column)\n",
    "        more_Q3 = 'more_Q3_{}'.format(column)\n",
    "        Q1 = 'Q1_{}'.format(column)\n",
    "        Q3 = 'Q3_{}'.format(column)\n",
    "\n",
    "        # Q1 : First Quartile ., Q3 : Third Quartile\n",
    "        Q1 = df.approxQuantile(column,[0.25],relativeError=0)\n",
    "        Q3 = df.approxQuantile(column,[0.75],relativeError=0)\n",
    "        \n",
    "        # IQR : Inter Quantile Range\n",
    "        # We need to define the index [0], as Q1 & Q3 are a set of lists., to perform a mathematical operation\n",
    "        # Q1 & Q3 are defined seperately so as to have a clear indication on First Quantile & 3rd Quantile\n",
    "        IQR = Q3[0] - Q1[0]\n",
    "        \n",
    "        #selecting the data, with -1.5*IQR to + 1.5*IQR., where param = 1.5 default value\n",
    "        less_Q1 =  Q1[0] - 1.5*IQR\n",
    "        more_Q3 =  Q3[0] + 1.5*IQR\n",
    "        \n",
    "        isOutlierCol = 'is_outlier_{}'.format(column)\n",
    "        \n",
    "        df = df.withColumn(isOutlierCol,when((df[column] > more_Q3) | (df[column] < less_Q1), 1).otherwise(0))\n",
    "    \n",
    "\n",
    "    # Selecting the specific columns which we have added above, to check if there are any outliers\n",
    "    selected_columns = [column for column in df.columns if column.startswith(\"is_outlier\")]\n",
    "    # Adding all the outlier columns into a new colum \"total_outliers\", to see the total number of outliers\n",
    "    df = df.withColumn('total_outliers',reduce(column_add, ( df[col] for col in  selected_columns)))\n",
    "\n",
    "    # Dropping the extra columns created above, just to create nice dataframe., without extra columns\n",
    "    df = df.drop(*[column for column in df.columns if column.startswith(\"is_outlier\")])\n",
    "\n",
    "    return df\n",
    "\n",
    "games_with_outliers = find_outliers(games_df_imputed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db6d71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10022\n",
      "9587\n"
     ]
    }
   ],
   "source": [
    "# plays_with_outliers.show(5, vertical=True)\n",
    "#allows some outliers so we have enough data\n",
    "print(games_with_outliers.count())\n",
    "games_without_outliers = games_with_outliers.filter(games_with_outliers[\"total_outliers\"] <4)\n",
    "print(games_without_outliers.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b93bde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playid: string (nullable = true)\n",
      " |-- gameid2: string (nullable = true)\n",
      " |-- IPcount: long (nullable = false)\n",
      " |-- SpecialTeamsPlayType: string (nullable = true)\n",
      " |-- possessionTeam: string (nullable = true)\n",
      " |-- offenseFormation: string (nullable = true)\n",
      " |-- personnel_off: string (nullable = true)\n",
      " |-- personnel_def: string (nullable = true)\n",
      " |-- yardlineSide: string (nullable = true)\n",
      " |-- KickReturn: integer (nullable = true)\n",
      " |-- number_PassRushers: integer (nullable = true)\n",
      " |-- defenders_InTheBox: integer (nullable = true)\n",
      " |-- YardAfterCatch: integer (nullable = true)\n",
      " |-- Pass_Length: integer (nullable = true)\n",
      " |-- yardsToGo: integer (nullable = true)\n",
      " |-- HomeScore: integer (nullable = true)\n",
      " |-- VisitorScore: integer (nullable = true)\n",
      " |-- yardlin_number_imputed: integer (nullable = true)\n",
      " |-- isSTPlay_encoded: integer (nullable = true)\n",
      " |-- possessionTeam_index: double (nullable = false)\n",
      " |-- personnel_offense_index: double (nullable = false)\n",
      " |-- personnel_defense_index: double (nullable = false)\n",
      " |-- offenseFormation_index: double (nullable = false)\n",
      " |-- yardlineSide_index: double (nullable = false)\n",
      " |-- special_teamplay_index: double (nullable = false)\n",
      " |-- special_teamplay_encoded: vector (nullable = true)\n",
      " |-- possessionTeam_encoded: vector (nullable = true)\n",
      " |-- personnel_offense_encoded: vector (nullable = true)\n",
      " |-- personnel_defense_encoded: vector (nullable = true)\n",
      " |-- offenseFormation_encoded: vector (nullable = true)\n",
      " |-- yardlineSide_encoded: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#handling categorical variables\n",
    "games_df_without_binary = (games_without_outliers.drop(\"total_outliers\")\\\n",
    "                                .withColumn(\"isSTPlay_encoded\", \n",
    "                                            games_without_outliers[\"isSTPlay\"].cast(\"integer\")).drop(\"isSTPlay\"))\n",
    "\n",
    "#create pipeline to index categorical values\n",
    "stage_1 = StringIndexer(inputCol= 'possessionTeam', outputCol= 'possessionTeam_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_2 = StringIndexer(inputCol= 'personnel_off', outputCol= 'personnel_offense_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_3 = StringIndexer(inputCol= 'personnel_def', outputCol= 'personnel_defense_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_4 = StringIndexer(inputCol= 'offenseFormation', outputCol= 'offenseFormation_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_5 = StringIndexer(inputCol= 'yardlineSide', outputCol= 'yardlineSide_index', handleInvalid=\"keep\")\n",
    "\n",
    "stage_6 = StringIndexer(inputCol='SpecialTeamsPlayType',outputCol = 'special_teamplay_index',handleInvalid = \"keep\")\n",
    "\n",
    "\n",
    "\n",
    "# define one hot encode of the numeric columns\n",
    "stage_7 = OneHotEncoder(inputCols=['special_teamplay_index','possessionTeam_index','personnel_offense_index','personnel_defense_index',\n",
    "                                  'offenseFormation_index','yardlineSide_index'], \n",
    "                        outputCols=['special_teamplay_encoded','possessionTeam_encoded','personnel_offense_encoded','personnel_defense_encoded',\n",
    "                                   'offenseFormation_encoded','yardlineSide_encoded'])\n",
    "\n",
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages=[stage_1, stage_2, stage_3, stage_4, stage_5, stage_6, stage_7])\n",
    "\n",
    "# fit the pipeline model and transform the data as defined\n",
    "pipeline_model = pipeline.fit(games_df_without_binary)\n",
    "encoded_games_df = pipeline_model.transform(games_df_without_binary)\n",
    "\n",
    "# view the transformed data\n",
    "\n",
    "encoded_games_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e882153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- playid: string (nullable = true)\n",
      " |-- gameid2: string (nullable = true)\n",
      " |-- IPcount: long (nullable = false)\n",
      " |-- SpecialTeamsPlayType: string (nullable = true)\n",
      " |-- possessionTeam: string (nullable = true)\n",
      " |-- offenseFormation: string (nullable = true)\n",
      " |-- personnel_off: string (nullable = true)\n",
      " |-- personnel_def: string (nullable = true)\n",
      " |-- yardlineSide: string (nullable = true)\n",
      " |-- KickReturn: integer (nullable = true)\n",
      " |-- number_PassRushers: integer (nullable = true)\n",
      " |-- defenders_InTheBox: integer (nullable = true)\n",
      " |-- YardAfterCatch: integer (nullable = true)\n",
      " |-- Pass_Length: integer (nullable = true)\n",
      " |-- yardsToGo: integer (nullable = true)\n",
      " |-- HomeScore: integer (nullable = true)\n",
      " |-- VisitorScore: integer (nullable = true)\n",
      " |-- yardlin_number_imputed: integer (nullable = true)\n",
      " |-- isSTPlay_encoded: integer (nullable = true)\n",
      " |-- possessionTeam_index: double (nullable = false)\n",
      " |-- personnel_offense_index: double (nullable = false)\n",
      " |-- personnel_defense_index: double (nullable = false)\n",
      " |-- offenseFormation_index: double (nullable = false)\n",
      " |-- yardlineSide_index: double (nullable = false)\n",
      " |-- special_teamplay_index: double (nullable = false)\n",
      " |-- special_teamplay_encoded: vector (nullable = true)\n",
      " |-- possessionTeam_encoded: vector (nullable = true)\n",
      " |-- personnel_offense_encoded: vector (nullable = true)\n",
      " |-- personnel_defense_encoded: vector (nullable = true)\n",
      " |-- offenseFormation_encoded: vector (nullable = true)\n",
      " |-- yardlineSide_encoded: vector (nullable = true)\n",
      " |-- vectorized_features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "vector_assembler = VectorAssembler(\n",
    "    inputCols=['special_teamplay_encoded','possessionTeam_encoded','personnel_offense_encoded','personnel_defense_encoded',\n",
    "               'offenseFormation_encoded','yardlineSide_encoded','isSTPlay_encoded',\n",
    "                              'yardsToGo', \"yardlin_number_imputed\",\"Pass_Length\",\"KickReturn\",\n",
    "               \"defenders_InTheBox\",\"number_PassRushers\",\"YardAfterCatch\"], \n",
    "    outputCol=\"vectorized_features\")\n",
    "\n",
    "assembled_games_df = vector_assembler.transform(encoded_games_df)\n",
    "\n",
    "# view the transformed data\n",
    "assembled_games_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a79d286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records for training purposes is: 7633\n",
      "Number of records for testing purposes is: 1954\n"
     ]
    }
   ],
   "source": [
    "#create ML model data\n",
    "games_model_df = assembled_games_df.withColumn(\"outcome\", col(\"IPcount\").cast(\"int\")).drop(\"IPcount\")\n",
    "train, test = games_model_df.randomSplit([0.8, 0.2], seed = 1125)\n",
    "print(\"Number of records for training purposes is: \" + str(train.count()))\n",
    "print(\"Number of records for testing purposes is: \" + str(test.count()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "88b2bcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R Squared (R2) of linear regression model on test data = 0.348494\n",
      "R Squared (R2) of dicision tree model on test data = 0.724888\n"
     ]
    }
   ],
   "source": [
    "#since outcome is a continuous variable, we can do regression \n",
    "#use cross validation to find the best hyper parameters\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "#linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "lr = LinearRegression(featuresCol='vectorized_features',labelCol='outcome')\n",
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"outcome\",metricName=\"r2\")\n",
    "# Create ParamGrid for Cross Validation\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.1, 0.5])# regularization parameter\n",
    "             .addGrid(lr.maxIter, [5, 10, 15])#Number of iterations\n",
    "             .addGrid(lr.elasticNetParam, [0.1,0.3,0.8])\n",
    "             .build())\n",
    "cv = CrossValidator(estimator=lr, estimatorParamMaps=paramGrid, \n",
    "                    evaluator=lr_evaluator, numFolds=5)\n",
    "lrModel = cv.fit(train)\n",
    "lr_predictions = lrModel.transform(test)\n",
    "\n",
    "\n",
    "\n",
    "print(\"R Squared (R2) of linear regression model on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n",
    "\n",
    "#decision tree regressor\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "gbt = GBTRegressor(featuresCol = 'vectorized_features', labelCol = 'outcome')\n",
    "\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"outcome\", predictionCol=\"prediction\", metricName=\"r2\")\n",
    "param_grid = (ParamGridBuilder().addGrid(gbt.maxDepth,[5,7,9]).addGrid(gbt.maxIter, [20,30,40]).build())\n",
    "gbt_cv  = CrossValidator(estimator = gbt,estimatorParamMaps=param_grid,evaluator=gbt_evaluator,numFolds=5)\n",
    "gbt_model = gbt_cv.fit(train)\n",
    "\n",
    "gbt_predictions = gbt_model.transform(test)\n",
    "\n",
    "gbt_r2 = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"R Squared (R2) of decision tree model on test data = %g\" % gbt_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b5400e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error for linear regression model is  2.662053187105004\n",
      "Accuracy of linear regression model is  :  0.9329580348004094\n",
      "Mean squared error for decision tree model is : 1.7298639075686468\n",
      "Accuracy of decision tree model is  :  0.9426816786079836\n"
     ]
    }
   ],
   "source": [
    "lr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n",
    "                 labelCol=\"outcome\",metricName=\"rmse\")\n",
    "lr_mse = lr_evaluator.evaluate(lr_predictions)\n",
    "\n",
    "print(\"Mean squared error for linear regression model is \", lr_mse)\n",
    "#here we define accuracy as the absolute differences between the predicted values and real values is less than 1.5 times of mean squared error\n",
    "lr_accuracy = lr_predictions.filter((lr_predictions.outcome - lr_predictions.prediction)< 1.5* lr_mse).count() / float(lr_predictions.count())\n",
    "print(\"Accuracy of linear regression model is  : \",lr_accuracy)\n",
    "\n",
    "gbt_evaluator = RegressionEvaluator(\n",
    "    labelCol=\"outcome\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "gbt_rmse = gbt_evaluator.evaluate(gbt_predictions)\n",
    "print(\"Mean squared error for decision tree model is :\",gbt_rmse)\n",
    "gbt_accuracy = gbt_predictions.filter((gbt_predictions.outcome - gbt_predictions.prediction)<1.5*gbt_rmse).count()/float(gbt_predictions.count())\n",
    "print(\"Accuracy of decision tree model is  : \",gbt_accuracy)\n",
    "\n",
    "#gradient boost tree model has smaller mean squared error and higher accuracy, thus this is the better model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "163bcd99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cacheNodeIds: If false, the algorithm will pass trees to executors to match instances with nodes. If true, the algorithm will cache node IDs for each instance. Caching can speed up training of deeper trees. Users can set how often should the cache be checkpointed or disable it by setting checkpointInterval. (default: False)',\n",
       " 'checkpointInterval: set checkpoint interval (>= 1) or disable checkpoint (-1). E.g. 10 means that the cache will get checkpointed every 10 iterations. Note: this setting will be ignored if the checkpoint directory is not set in the SparkContext. (default: 10)',\n",
       " \"featureSubsetStrategy: The number of features to consider for splits at each tree node. Supported options: 'auto' (choose automatically for task: If numTrees == 1, set to 'all'. If numTrees > 1 (forest), set to 'sqrt' for classification and to 'onethird' for regression), 'all' (use all features), 'onethird' (use 1/3 of the features), 'sqrt' (use sqrt(number of features)), 'log2' (use log2(number of features)), 'n' (when n is in the range (0, 1.0], use n * number of features. When n is in the range (1, number of features), use n features). default = 'auto' (default: all)\",\n",
       " 'featuresCol: features column name. (default: features, current: vectorized_features)',\n",
       " 'impurity: Criterion used for information gain calculation (case-insensitive). Supported options: variance (default: variance)',\n",
       " 'labelCol: label column name. (default: label, current: outcome)',\n",
       " 'leafCol: Leaf indices column name. Predicted leaf index of each instance in each tree by preorder. (default: )',\n",
       " 'lossType: Loss function which GBT tries to minimize (case-insensitive). Supported options: squared, absolute (default: squared)',\n",
       " 'maxBins: Max number of bins for discretizing continuous features.  Must be >=2 and >= number of categories for any categorical feature. (default: 32)',\n",
       " 'maxDepth: Maximum depth of the tree. (>= 0) E.g., depth 0 means 1 leaf node; depth 1 means 1 internal node + 2 leaf nodes. Must be in range [0, 30]. (default: 5, current: 9)',\n",
       " 'maxIter: max number of iterations (>= 0). (default: 20, current: 40)',\n",
       " 'maxMemoryInMB: Maximum memory in MB allocated to histogram aggregation. If too small, then 1 node will be split per iteration, and its aggregates may exceed this size. (default: 256)',\n",
       " 'minInfoGain: Minimum information gain for a split to be considered at a tree node. (default: 0.0)',\n",
       " 'minInstancesPerNode: Minimum number of instances each child must have after split. If a split causes the left or right child to have fewer than minInstancesPerNode, the split will be discarded as invalid. Should be >= 1. (default: 1)',\n",
       " 'minWeightFractionPerNode: Minimum fraction of the weighted sample count that each child must have after split. If a split causes the fraction of the total weight in the left or right child to be less than minWeightFractionPerNode, the split will be discarded as invalid. Should be in interval [0.0, 0.5). (default: 0.0)',\n",
       " 'predictionCol: prediction column name. (default: prediction)',\n",
       " 'seed: random seed. (default: -6682481135904123338)',\n",
       " 'stepSize: Step size (a.k.a. learning rate) in interval (0, 1] for shrinking the contribution of each estimator. (default: 0.1)',\n",
       " 'subsamplingRate: Fraction of the training data used for learning each decision tree, in range (0, 1]. (default: 1.0)',\n",
       " 'validationIndicatorCol: name of the column that indicates whether each row is for training or for validation. False indicates training; true indicates validation. (undefined)',\n",
       " 'validationTol: Threshold for stopping early when fit with validation is used. If the error rate on the validation input changes by less than the validationTol, then learning will stop early (before `maxIter`). This parameter is ignored when fit without validation is used. (default: 0.01)',\n",
       " 'weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in conclusion, the decision tree model is better and the best version of it\n",
    "#inludes 40 iterations, depth of 9.\n",
    "best_model=gbt_model.bestModel\n",
    "best_model.explainParams().split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5e5909",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
