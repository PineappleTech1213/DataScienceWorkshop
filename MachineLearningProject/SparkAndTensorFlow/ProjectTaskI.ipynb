{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "753d26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    " #import needed packages\n",
    "import pyspark\n",
    "from pyspark import SparkContext #for unabling to set up sc by yourself\n",
    "from pyspark.sql import SparkSession, SQLContext #spark dataframe = spark sql\n",
    "from pyspark.sql.functions import to_date, col, count,when,lit,rand,to_timestamp,udf\n",
    "from pyspark.sql.functions import concat_ws, substring,isnan,regexp_replace,concat,desc\n",
    "from pyspark.sql.functions import udf,regexp_extract, monotonically_increasing_id,size\n",
    "from pyspark.ml.feature import Imputer\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, FloatType, StringType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "#building spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "61028082",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Spark/spark-3.2.1-bin-hadoop3.2-scala2.13/python/pyspark/sql/context.py:77: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#building spark session\n",
    "appname = \"courseproject\" #define app name\n",
    "master = \"local\"\n",
    "\n",
    "config = pyspark.SparkConf().setAppName(appname)\\\n",
    ".setMaster(master) #we do not have any workers.\n",
    "\n",
    "\n",
    "#session\n",
    "#with sql context, create session from it\n",
    "sc  = SparkContext.getOrCreate(conf=config)\n",
    "sqlContext = SQLContext(sc)\n",
    "#don't create many sessions, take up too much room!\n",
    "sp_session = sqlContext.sparkSession.builder.getOrCreate();\n",
    "#export data into postgresql\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "import os\n",
    "db_properties={}\n",
    "db_properties['username']=\"postgres\"\n",
    "db_properties['password']=\"postgres\"\n",
    "# make sure to use the correct port number. These \n",
    "db_properties['url']= \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "db_properties['driver']=\"org.postgresql.Driver\"\n",
    "\n",
    "#define the function to store players data each year\n",
    "#take the name of the spark table and the name of the table to be created\n",
    "\n",
    "def write_to_postgres(df,table_name):\n",
    "     df.write.format(\"jdbc\")\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .option(\"url\", \"jdbc:postgresql://localhost:5432/postgres\")\\\n",
    "    .option(\"dbtable\", table_name)\\\n",
    "    .option(\"user\", \"postgres\")\\\n",
    "    .option(\"password\", \"postgres\")\\\n",
    "    .option(\"Driver\", \"org.postgresql.Driver\")\\\n",
    "    .save()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "21b39c10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#drop those columns have >50% empty cells\n",
    "#impute columns have < 50% empty cells\n",
    "#do not impute categorical columns\n",
    "#compute proper values for some columns\n",
    "#conduct the operations in some columns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#define the function to clean the data\n",
    "def clean_fifa(df,year):\n",
    "    #read in the data\n",
    "    players_df= (sp_session.read.format(\"csv\")\\\n",
    "                 .option(\"inferSchema\",\"true\")\\\n",
    "                 .option(\"header\",\"true\").load(df))\n",
    "    #cast some columns into correct data type\n",
    "    players_cast= players_df.withColumn(\"SofifaId\", col(\"sofifa_id\").cast(\"string\")).drop(\"sofifa_id\")\\\n",
    "    .withColumn(\"DOB\",to_date(col(\"dob\"))).withColumn(\"Year\", lit(year))\n",
    "    #find out columns that have too many null values\n",
    "    null_counts = players_cast.select([(count(when(isnan(c)| col(c).isNull(),c))/players_cast.count()).alias(c) \n",
    "                         for c in players_cast.drop(\"DOB\").columns]).collect()[0].asDict()\n",
    "    to_drop = [k for k, v in null_counts.items() if v > 0.5]\n",
    "    players_drop = players_cast.drop(*to_drop)\n",
    "#     drop columns that have less than 50% values\n",
    "    print(\"dropped empty columns\")\n",
    "#     players_drop.printSchema()\n",
    "    #compute proper values for some columns\n",
    "    players_int_col = players_drop.select(\"ls\",\"st\",\"rs\",\"lw\",\"lf\",\"cf\",\"rf\",\"rw\",\"lam\",\"cam\",\n",
    "                        \"ram\",\"lm\",\"lcm\",\"cm\",\"rcm\",\"rm\",\"lwb\",\"ldm\",\"cdm\",\n",
    "                        \"rdm\",\"rwb\",\"lb\",\"lcb\",\"cb\",\"rcb\",\"rb\",\"gk\").columns\n",
    "    for column in players_int_col:\n",
    "        int_full = 'full_{}'.format(column) \n",
    "        int_first = 'first_{}'.format(column)\n",
    "        int_operand = 'op_{}'.format(column)\n",
    "        int_second = 'second_{}'.format(column)\n",
    "        int_value = \"cleaned_{}\".format(column)\n",
    "        players_drop = players_drop.withColumn(int_full,when(col(column).cast(\"int\").isNotNull(),col(column).cast(\"int\")).otherwise(0))\\\n",
    "        .withColumn(int_first,when(regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',1).cast(\"int\").isNotNull(),\n",
    "                                     regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',1).cast(\"int\")).otherwise(0))\\\n",
    "        .withColumn(int_operand,when(regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',2).isNotNull(),\n",
    "                                     regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',2)).otherwise(\"null\"))\\\n",
    "        .withColumn(int_second,when(regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',3).cast(\"int\").isNotNull(),\n",
    "                                     regexp_extract(col(column),'([0-9]*)([-+])([0-9]*)',3).cast(\"int\")).otherwise(0))\\\n",
    "        .withColumn(int_value,when(col(int_operand) ==\"+\",\n",
    "                                     col(int_first)+col(int_second)).when(col(int_operand) ==\"-\",col(int_first)-col(int_second)).otherwise(col(int_full)))\\\n",
    "        .drop(int_operand,int_full,int_first,int_second,column)\n",
    "    print(\"numeric data have been cleaned.\")\n",
    "#     players_drop.show(10)\n",
    "    #impute the data\n",
    "    na_col = [\"value_eur\",\"wage_eur\",\"pace\",\"shooting\",\"passing\",\"dribbling\"]\n",
    "    na_imp_col = [\"{}_imputed\".format(c) for c in na_col]\n",
    "    player_imputer = Imputer(inputCols = na_col,\n",
    "                             outputCols = na_imp_col).setMissingValue(0).setStrategy(\"mean\")\n",
    "    players_imputed = player_imputer.fit(players_drop)\\\n",
    "    .transform(players_drop)\\\n",
    "    .drop(\"value_eur\",\"wage_eur\",\"pace\",\"shooting\",\"passing\",\"dribbling\")\n",
    "    print(\"finishing imputation.\")\n",
    "                                         \n",
    "    tablename = \"FIFA.PLAYERS_\"+str(year)\n",
    "    write_to_postgres(players_imputed, tablename)\n",
    "    print(\"ingested into postgres.\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "727acdde",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../Data/players_15.csv\",15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0780df49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_16.csv\",16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "738c140a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_17.csv\",17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "41f78f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_18.csv\",18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "984cdcdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_19.csv\",19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1df5c738",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_20.csv\",20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "64dd17e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_21.csv\",21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9d177351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped empty columns\n",
      "numeric data have been cleaned.\n",
      "finishing imputation.\n",
      "ingested into postgres.\n"
     ]
    }
   ],
   "source": [
    "clean_fifa(\"../data/players_22.csv\",22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7ca5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end of Task1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
